# Limbic-Flow AI æ¨¡å—æ–‡æ¡£

## ğŸ“– æ¦‚è¿°

`core/ai` æ¨¡å—æ˜¯ Limbic-Flow çš„ LLM æŠ½è±¡å±‚ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨å¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å—éµå¾ª**ä¾èµ–å€’ç½®åŸåˆ™**å’Œ**å¼€é—­åŸåˆ™**ï¼Œå®ç°äº†é«˜åº¦è§£è€¦ã€å¯ç»´æŠ¤ã€å¯æ‰©å±•çš„æ¶æ„ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç›®å½•ç»“æ„

```
core/ai/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base.py              # LLM æŠ½è±¡åŸºç±»
â”œâ”€â”€ factory.py           # LLM å·¥å‚ç±»
â””â”€â”€ adapters/            # å„å‚å•†é€‚é…å™¨
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ openai.py        # OpenAI é€‚é…å™¨
    â”œâ”€â”€ deepseek.py      # DeepSeek é€‚é…å™¨
    â”œâ”€â”€ anthropic.py     # Anthropic é€‚é…å™¨
    â””â”€â”€ ollama.py        # Ollama é€‚é…å™¨
```

### æ ¸å¿ƒç»„ä»¶

#### 1. **BaseLLMï¼ˆæŠ½è±¡åŸºç±»ï¼‰**

å®šä¹‰äº†æ‰€æœ‰ LLM é€‚é…å™¨å¿…é¡»å®ç°çš„æ¥å£ï¼š

```python
class BaseLLM(ABC):
    @abstractmethod
    def chat(self, messages: List[Message], **kwargs) -> LLMResponse:
        """èŠå¤©æ¥å£ - æ ¸å¿ƒæ–¹æ³•"""
        pass
```

**ä¸»è¦æ–¹æ³•ï¼š**
- `chat()`: å¤šè½®å¯¹è¯æ¥å£
- `chat_simple()`: ç®€åŒ–çš„å•è½®å¯¹è¯æ¥å£
- `health_check()`: å¥åº·æ£€æŸ¥

#### 2. **LLMFactoryï¼ˆå·¥å‚ç±»ï¼‰**

è´Ÿè´£åˆ›å»ºå’Œç®¡ç† LLM å®ä¾‹ï¼š

```python
factory = LLMFactory()
llm = factory.create_llm("openai")
```

**ä¸»è¦æ–¹æ³•ï¼š**
- `create_llm()`: åˆ›å»º LLM å®ä¾‹
- `register_llm()`: æ³¨å†Œæ–°çš„ LLM æä¾›å•†
- `get_supported_providers()`: è·å–æ”¯æŒçš„æä¾›å•†åˆ—è¡¨

#### 3. **é€‚é…å™¨ï¼ˆAdaptersï¼‰**

æ¯ä¸ªé€‚é…å™¨è´Ÿè´£å¯¹æ¥ä¸€ä¸ªå‚å•†çš„ APIï¼š

- **OpenAILLM**: æ”¯æŒ GPT-4ã€GPT-3.5-turbo
- **DeepSeekLLM**: æ”¯æŒ deepseek-chatã€deepseek-coder
- **AnthropicLLM**: æ”¯æŒ Claude 3 ç³»åˆ—
- **OllamaLLM**: æ”¯æŒæ‰€æœ‰ Ollama æ¨¡å‹ï¼ˆæœ¬åœ°è¿è¡Œï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -e .
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` ä¸º `.env` å¹¶å¡«å…¥ API Keysï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```env
# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat

# é»˜è®¤æä¾›å•†
DEFAULT_LLM_PROVIDER=openai
```

### 3. åŸºæœ¬ä½¿ç”¨

```python
from limbic_flow.core.ai import LLMFactory

# åˆ›å»ºå·¥å‚å®ä¾‹
factory = LLMFactory()

# åˆ›å»º LLM å®ä¾‹ï¼ˆä½¿ç”¨é»˜è®¤æä¾›å•†ï¼‰
llm = factory.create_llm()

# å‘é€æ¶ˆæ¯
response = llm.chat_simple("ä½ å¥½ï¼")
print(response.content)
```

### 4. æŒ‡å®šæä¾›å•†

```python
# ä½¿ç”¨ OpenAI
openai_llm = factory.create_llm("openai")

# ä½¿ç”¨ DeepSeek
deepseek_llm = factory.create_llm("deepseek")

# ä½¿ç”¨ Anthropic
anthropic_llm = factory.create_llm("anthropic")

# ä½¿ç”¨ Ollamaï¼ˆæœ¬åœ°ï¼‰
ollama_llm = factory.create_llm("ollama")
```

## ğŸ“š API å‚è€ƒ

### LLMConfig

LLM é…ç½®ç±»ï¼š

```python
@dataclass
class LLMConfig:
    model: str                      # æ¨¡å‹åç§°
    api_key: Optional[str] = None   # API Key
    base_url: Optional[str] = None  # API åŸºç¡€ URL
    temperature: float = 0.7        # æ¸©åº¦å‚æ•°
    max_tokens: Optional[int] = None # æœ€å¤§ token æ•°
    timeout: int = 30               # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    extra_params: Optional[Dict[str, Any]] = None  # é¢å¤–å‚æ•°
```

### Message

æ¶ˆæ¯ç±»ï¼š

```python
@dataclass
class Message:
    role: MessageRole    # æ¶ˆæ¯è§’è‰²ï¼ˆSYSTEM, USER, ASSISTANTï¼‰
    content: str         # æ¶ˆæ¯å†…å®¹
```

### LLMResponse

LLM å“åº”ç±»ï¼š

```python
@dataclass
class LLMResponse:
    content: str                        # å“åº”å†…å®¹
    model: str                          # ä½¿ç”¨çš„æ¨¡å‹
    usage: Optional[Dict[str, int]]    # Token ä½¿ç”¨æƒ…å†µ
    raw_response: Optional[Dict[str, Any]]  # åŸå§‹å“åº”
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. å¤šè½®å¯¹è¯

```python
from limbic_flow.core.ai import LLMFactory, Message, MessageRole

factory = LLMFactory()
llm = factory.create_llm("openai")

messages = [
    Message(role=MessageRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"),
    Message(role=MessageRole.USER, content="ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"),
    Message(role=MessageRole.ASSISTANT, content="Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€..."),
    Message(role=MessageRole.USER, content="å®ƒæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"),
]

response = llm.chat(messages)
print(response.content)
```

### 2. è‡ªå®šä¹‰å‚æ•°

```python
response = llm.chat_simple(
    prompt="å†™ä¸€ä¸ªæ•…äº‹",
    temperature=0.9,      # æ›´æœ‰åˆ›æ„
    max_tokens=1000       # æ›´é•¿çš„è¾“å‡º
)
```

### 3. æ³¨å†Œæ–°çš„ LLM æä¾›å•†

```python
from limbic_flow.core.ai import BaseLLM, LLMFactory, LLMConfig

class CustomLLM(BaseLLM):
    def _initialize_client(self):
        # åˆå§‹åŒ–ä½ çš„å®¢æˆ·ç«¯
        pass
    
    def chat(self, messages, **kwargs):
        # å®ç°èŠå¤©é€»è¾‘
        pass

# æ³¨å†Œæ–°æä¾›å•†
LLMFactory.register_llm("custom", CustomLLM)

# ä½¿ç”¨æ–°æä¾›å•†
factory = LLMFactory()
custom_llm = factory.create_llm("custom")
```

### 4. åœ¨ Limbic-Flow Pipeline ä¸­ä½¿ç”¨

```python
from limbic_flow.pipeline import LimbicFlowPipeline

# ä½¿ç”¨ OpenAI
pipeline = LimbicFlowPipeline(llm_provider="openai")

# ä½¿ç”¨ DeepSeek
pipeline = LimbicFlowPipeline(llm_provider="deepseek")

# å¤„ç†è¾“å…¥
result = pipeline.process_input("æˆ‘ä»Šå¤©æ„Ÿè§‰å¾ˆå¼€å¿ƒï¼")
print(result["response"])
```

## ğŸ¯ æ”¯æŒçš„ LLM æä¾›å•†

| æä¾›å•† | æ¨¡å‹ | éœ€è¦çš„é…ç½® |
|--------|------|------------|
| OpenAI | gpt-4-turbo-preview, gpt-3.5-turbo | `OPENAI_API_KEY` |
| DeepSeek | deepseek-chat, deepseek-coder | `DEEPSEEK_API_KEY` |
| Anthropic | claude-3-opus-20240229, claude-3-sonnet-20240229 | `ANTHROPIC_API_KEY` |
| Ollama | llama2, mistral, codellama ç­‰ | æ— éœ€ API Keyï¼ˆæœ¬åœ°è¿è¡Œï¼‰ |

## ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ

1. **æ°¸è¿œä¸è¦å°† API Key æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ**
   - ä½¿ç”¨ `.env` æ–‡ä»¶
   - å°† `.env` æ·»åŠ åˆ° `.gitignore`

2. **ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯**
   ```python
   import os
   api_key = os.getenv("OPENAI_API_KEY")
   ```

3. **åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡**
   - AWS Secrets Manager
   - Azure Key Vault
   - HashiCorp Vault

## ğŸ› æ•…éšœæ’æŸ¥

### 1. API Key é”™è¯¯

```
ValueError: OpenAI API Key ä¸èƒ½ä¸ºç©º
```

**è§£å†³æ–¹æ¡ˆ**ï¼šæ£€æŸ¥ `.env` æ–‡ä»¶ä¸­çš„ API Key æ˜¯å¦æ­£ç¡®é…ç½®ã€‚

### 2. è¿æ¥è¶…æ—¶

```
Exception: API è°ƒç”¨å¤±è´¥: timeout
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- å¢åŠ  `timeout` å‚æ•°
- æ£€æŸ¥ API åŸºç¡€ URL æ˜¯å¦æ­£ç¡®

### 3. æ¨¡å‹ä¸å­˜åœ¨

```
Exception: Model not found
```

**è§£å†³æ–¹æ¡ˆ**ï¼šæ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œå‚è€ƒæ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ã€‚

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

1. **ä½¿ç”¨åˆé€‚çš„æ¨¡å‹**ï¼š
   - ç®€å•ä»»åŠ¡ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ gpt-3.5-turboï¼‰
   - å¤æ‚ä»»åŠ¡ä½¿ç”¨è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå¦‚ gpt-4-turbo-previewï¼‰

2. **è°ƒæ•´æ¸©åº¦å‚æ•°**ï¼š
   - åˆ›æ„ä»»åŠ¡ä½¿ç”¨è¾ƒé«˜çš„æ¸©åº¦ï¼ˆ0.8-1.0ï¼‰
   - ç²¾ç¡®ä»»åŠ¡ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ï¼ˆ0.1-0.3ï¼‰

3. **é™åˆ¶è¾“å‡ºé•¿åº¦**ï¼š
   - è®¾ç½®åˆç†çš„ `max_tokens` ä»¥æ§åˆ¶æˆæœ¬

## ğŸ¤ è´¡çŒ®æŒ‡å—

å¦‚æœä½ æƒ³è¦æ·»åŠ æ–°çš„ LLM æä¾›å•†æ”¯æŒï¼š

1. åœ¨ `core/ai/adapters/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„é€‚é…å™¨æ–‡ä»¶
2. ç»§æ‰¿ `BaseLLM` ç±»
3. å®ç°å¿…éœ€çš„æ–¹æ³•
4. åœ¨ `factory.py` ä¸­æ³¨å†Œæ–°çš„æä¾›å•†
5. æ›´æ–°æ–‡æ¡£

## ğŸ“ è®¸å¯è¯

MIT License
