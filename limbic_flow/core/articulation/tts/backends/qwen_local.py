from typing import Dict, Optional, Any
import os
import torch
import soundfile as sf
import asyncio
from limbic_flow.core.articulation.tts.base import TTSBackend

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœç¯å¢ƒæ²¡å‡†å¤‡å¥½åˆ™æŠ¥é”™
try:
    from qwen_tts import Qwen3TTSModel
    HAS_QWEN = True
except ImportError:
    HAS_QWEN = False

class QwenLocalTTS(TTSBackend):
    """
    Qwen3-TTS æœ¬åœ°æ¨¡å‹åç«¯
    ä½¿ç”¨ Qwen3-TTS-12Hz-1.7B-VoiceDesign è¿›è¡ŒåŸºäºæŒ‡ä»¤çš„è¯­éŸ³ç”Ÿæˆ
    """

    def __init__(self, model_path: str = "Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign", device: str = "cuda:0"):
        if not HAS_QWEN:
            raise ImportError("è¯·å…ˆå®‰è£… qwen-tts: `pip install -U qwen-tts`")

        print(f"ğŸ”„ [QwenTTS] Loading model from {model_path} on {device}...")
        self.model = Qwen3TTSModel.from_pretrained(
            model_path,
            device_map=device,
            dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )
        print("âœ… [QwenTTS] Model loaded successfully.")

    @property
    def provider_name(self) -> str:
        return "qwen_local"

    async def generate(self, text: str, output_path: str, emotion_state: Optional[Dict[str, float]] = None) -> str:
        """
        ç”Ÿæˆè¯­éŸ³æ–‡ä»¶
        """
        # 1. æ„å»ºæŒ‡ä»¤ (Instruction)
        instruct = self._build_instruct_from_emotion(emotion_state or {})
        print(f"ğŸ™ï¸ [QwenTTS] Generating with instruct: '{instruct}'")

        # 2. è¿è¡Œæ¨ç† (åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œä»¥é˜²é˜»å¡äº‹ä»¶å¾ªç¯)
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ run_in_executor
        try:
            wavs, sr = self.model.generate_voice_design(
                text=text,
                language="Chinese", # é»˜è®¤ä¸­æ–‡ï¼Œæœªæ¥å¯é…ç½®
                instruct=instruct
            )

            # 3. ä¿å­˜æ–‡ä»¶
            sf.write(output_path, wavs[0], sr)
            return output_path

        except Exception as e:
            print(f"âŒ [QwenTTS] Generation failed: {e}")
            raise

    async def speak(self, text: str, emotion_state: Optional[Dict[str, float]] = None):
        """
        ç”Ÿæˆå¹¶æ’­æ”¾
        """
        # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶
        temp_path = "temp_speech.wav"
        await self.generate(text, temp_path, emotion_state)

        # æ’­æ”¾ (ä½¿ç”¨ç®€å•çš„ç³»ç»Ÿå‘½ä»¤ï¼Œè·¨å¹³å°å¯èƒ½éœ€è¦è°ƒæ•´)
        print(f"ğŸ”Š [QwenTTS] Playing audio...")
        if os.name == 'posix': # Mac/Linux
            os.system(f"afplay {temp_path}" if os.uname().sysname == 'Darwin' else f"aplay {temp_path}")
        else: # Windows
            # Windows æ’­æ”¾ wav æœ‰ç‚¹éº»çƒ¦ï¼Œå¯ä»¥ç”¨ powershell
            os.system(f'powershell -c (New-Object Media.SoundPlayer "{temp_path}").PlaySync();')

    def _build_instruct_from_emotion(self, emotion: Dict[str, float]) -> str:
        """
        å°† PAD æƒ…ç»ªå€¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ (Prompt Engineering for Audio)
        """
        pleasure = emotion.get('pleasure', 0.0)
        arousal = emotion.get('arousal', 0.0)
        dominance = emotion.get('dominance', 0.0)

        # åŸºç¡€éŸ³è‰²è®¾å®š (å¯ä»¥åšæˆå¯é…ç½®çš„)
        base_voice = "å¹´è½»å¥³æ€§å£°éŸ³ï¼ŒéŸ³è‰²æ¸…æ¾ˆè‡ªç„¶"

        descriptors = []

        # Pleasure (æ„‰æ‚¦åº¦)
        if pleasure > 0.6:
            descriptors.append("å……æ»¡å–œæ‚¦å’Œçƒ­æƒ…")
        elif pleasure > 0.2:
            descriptors.append("è¯­æ°”è½»æ¾æ„‰å¿«")
        elif pleasure < -0.6:
            descriptors.append("æåº¦æ‚²ä¼¤ï¼Œå¸¦æœ‰å“­è…”")
        elif pleasure < -0.2:
            descriptors.append("è¯­æ°”ä½è½å¿§éƒ")

        # Arousal (å”¤é†’åº¦) - å½±å“è¯­é€Ÿå’Œèƒ½é‡
        if arousal > 0.6:
            descriptors.append("è¯­é€Ÿè¾ƒå¿«ï¼Œèƒ½é‡å……æ²›ï¼Œæƒ…ç»ªæ¿€åŠ¨")
        elif arousal > 0.2:
            descriptors.append("è¯­é€Ÿè½»å¿«")
        elif arousal < -0.6:
            descriptors.append("è¯­é€Ÿç¼“æ…¢ï¼Œæœ‰æ°”æ— åŠ›ï¼Œç”šè‡³æœ‰åœé¡¿")
        elif arousal < -0.2:
            descriptors.append("è¯­é€Ÿèˆ’ç¼“å¹³é™")

        # Dominance (æ§åˆ¶åº¦) - å½±å“è‡ªä¿¡å’Œè¯­æ°”å¼ºå¼±
        if dominance > 0.5:
            descriptors.append("è¯­æ°”åšå®šè‡ªä¿¡ï¼Œä¸å®¹ç½®ç–‘")
        elif dominance < -0.5:
            descriptors.append("è¯­æ°”æ€¯æ‡¦ï¼Œå°å¿ƒç¿¼ç¿¼ï¼Œç¼ºä¹è‡ªä¿¡")

        # ç»„åˆæŒ‡ä»¤
        instruct_suffix = "ï¼Œ".join(descriptors)
        if not instruct_suffix:
            instruct_suffix = "è¯­æ°”å¹³å’Œè‡ªç„¶"

        return f"{base_voice}ï¼Œ{instruct_suffix}ã€‚"
